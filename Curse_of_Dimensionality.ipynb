{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuJ4FAGnc6ZpjPz4Zfq30x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushyash24/Machine_Learning/blob/main/Curse_of_Dimensionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Curse of Dimenionality**\n",
        "\n",
        "The curse of dimensionality refers to the set of problems that arise when the number of features (dimensions) increases, making data sparse, models inefficient, and learning difficult."
      ],
      "metadata": {
        "id": "YUIXcpcxy0ii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO7jlyRSwQIS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t is called a curse because as the number of features increases, the data points become very sparse and spread far apart in the feature space, making it difficult to find meaningful patterns. Distance-based measures start failing because the distance between the nearest and the farthest data points becomes almost the same, which negatively affects algorithms like KNN, K-means, and DBSCAN. Additionally, the amount of data required to properly represent the feature space grows exponentially with dimensions, which is practically not possible to collect. High dimensional data also leads to overfitting, where the model learns noise instead of actual patterns, resulting in high training accuracy but poor testing accuracy. Moreover, computational cost increases significantly, causing higher training time and greater memory usage.\n",
        "\n",
        "Algorithms Affected\n",
        "The curse of dimensionality mainly affects distance-based and tree-based algorithms. These include K-Nearest Neighbors (KNN), K-means clustering, Hierarchical Clustering, DBSCAN, and deep Decision Trees, as their performance heavily depends on distance calculations or feature splits.\n",
        "\n",
        "Example\n",
        "In low-dimensional data, a small number of data points is usually sufficient to represent the space and learn patterns effectively. However, as dimensions increase, the same number of data points becomes inadequate because the volume of the feature space increases rapidly, making the data extremely sparse.\n",
        "\n",
        "How to Handle the Curse of Dimensionality\n",
        "The curse of dimensionality can be handled using several techniques. Feature selection helps by removing irrelevant and redundant features. Dimensionality reduction techniques such as PCA, LDA, and autoencoders reduce the number of features while preserving important information. Feature engineering methods like feature construction, feature splitting, and feature scaling also improve learning efficiency. Regularization techniques such as L1 (Lasso) and L2 (Ridge) penalize model complexity and help prevent overfitting."
      ],
      "metadata": {
        "id": "gIYrTdK0zWax"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fb6BiwuAzFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jab features zyada ho jaate hain to data space bahut bada ho jaata hai, jisse model ko pattern samajhna mushkil ho jaata hai. Is problem ko curse of dimensionality kehte hain."
      ],
      "metadata": {
        "id": "icauKC62zMop"
      }
    }
  ]
}